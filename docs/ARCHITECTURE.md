# Hybrid Intelligence Architecture
**Modular • Code-First • Protocol-Driven**

## 1. High-Level Overview

This system is a **Hybrid Router Architecture** designed for high modularity and secure tool execution. It decouples the chatbot's "Brain" (LLM) from its "Limbs" (Tools) using the **Model Context Protocol (MCP)**.

### The Core Loop
1.  **Reflexes (Rasa):** Fast, deterministic handling of small talk and fixed intents.
2.  **Reasoning (LLM):** Generative handling of complex queries via Python code generation.
3.  **Orchestration (MCP Host):** A central switchboard that manages connections to isolated tool servers.


---

## 2. System Components

### A. The "Brain" & Traffic Control

#### **1. Hybrid Router (`hybrid_router.py`)**
The entry point for all user messages.
* **Role:** Traffic Controller.
* **Logic:**
    * Checks for **Form Submissions** (Adaptive Cards).
    * Checks for **Sticky Context** (Are we locked into a multi-turn tool flow?).
    * Queries **Rasa NLU**. If confidence > 70% (e.g., "Hi", "Bye"), Rasa handles it instantly.
    * If complex, routes to the **LLM Router**.

#### **2. LLM Router (`router.py`)**
The reasoning engine.
* **Role:** The Planner / CEO.
* **Logic:**
    * Receives the user query and conversation history.
    * Does *not* call tools directly.
    * **Generates Python Code** to solve the problem, orchestrating multiple steps if necessary.

### B. The "Operations Center" (The MCP Layer)

#### **3. MCP Host (`mcp_host.py`)**
The central **Switchboard** and **Manager**.
* **Role:** Operations Manager.
* **Key Responsibilities:**
    * **Connection Pooling:** Reads `mcp_servers.json` and maintains active connections to all external servers (Weather, DB, Filesystem).
    * **Unified Interface:** Exposes a single `call_tool("server.function")` method. The LLM doesn't need to know *how* to connect to the weather server; it just asks the Host.
    * **Tool Discovery:** Aggregates tools from all connected servers into a single directory.
    * **Parallel Execution:** Can execute multiple tool calls simultaneously (e.g., querying SQL and Vector DB at the same time) via `call_tools_parallel`.
    * **Progressive Disclosure:** Allows the LLM to `search_tools()` effectively, preventing context window overflow by only loading detailed schemas when needed.

#### **4. MCP Client (`mcp_client.py`)**
The individual connector.
* **Role:** The Phone Line.
* **Logic:** Manages the specific transport (Stdio/HTTP) to *one* MCP Server. It handles the JSON-RPC communication protocol.

### C. The "Hands" (Execution)

#### **5. Code Executor (`code_executor.py`)**
The secure sandbox.
* **Role:** The Workbench.
* **Logic:**
    * Takes the Python code generated by the LLM.
    * **Sandboxes it:** Blocks imports like `os` or `subprocess` to prevents hacking.
    * **Injects Magic Functions:** Gives the code access to `call_tool`, `search_tools`, and `print`.
    * executes the code and captures the output.

### D. Memory & State

#### **6. Conversation Memory (`conversation_memory.py`)**
* **Role:** The Hippocampus.
* **Backend:** Redis.
* **Features:** Tracks per-user sessions, stores tool outputs, and manages context windows.

---

## 3. Detailed Data Flow

### Scenario: "Get me the weather in London and save it to a file."

1.  **Ingestion:** User sends message.
2.  **Routing:** `HybridRouter` sees it's not small talk -> sends to **LLM Router**.
3.  **Prompting:** LLM receives a system prompt listing available tools (provided by **MCP Host**).
4.  **Code Generation:** LLM writes:
    ```python
    # 1. Get weather
    weather = await call_tool("weather.get_current", {"city": "London"})
    # 2. Save file
    await call_tool("filesystem.write_file", {
        "path": "weather.txt",
        "content": f"London: {weather['temp']}C"
    })
    print("Done!")
    ```
5.  **Execution:** `CodeExecutor` runs this block.
    * Line 1: Calls `call_tool`. **MCP Host** routes this to the **Weather Client**.
    * Line 2: Calls `call_tool`. **MCP Host** routes this to the **Filesystem Client**.
6.  **Response:** The execution result ("Done!") and logs are sent back to the LLM to formulate a final natural language response.

---

## 4. Key Architectural Patterns

### 1. The "Switchboard" Pattern (MCP Host)
Instead of the application managing 10 different APIs individually, the **MCP Host** acts as a unified gateway.
* **Benefit:** You can add a new tool (e.g., "Stock Market") just by adding an entry to `mcp_servers.json`. The Host automatically connects, indexes its tools, and makes them available to the LLM. No code changes in `router.py` or `code_executor.py` are required.

### 2. Code-First Tool Use
Instead of the LLM outputting a single JSON object for one tool call (the standard OpenAI function calling pattern), the LLM outputs **Python Code**.
* **Benefit:** Complex logic (loops, data transformation, if/else conditions) happens inside the sandbox in a *single turn*, rather than requiring 5 back-and-forth network calls with the user.

### 3. Progressive Disclosure
The **MCP Host** supports `search_tools`.
* **Benefit:** If you have 1,000 tools, you don't stuff them all into the LLM's prompt (which costs money and confuses the model). The LLM sees a high-level tree, and can "search" for the specific tool it needs, loading the full definition only on demand.

### 4. Skill Persistence
The **MCP Host** includes a `save_skill` mechanism.
* **Benefit:** If the LLM figures out a complex way to "Calculate Mortgage" using 3 different tools, it can save that Python snippet as a reusable "Skill." Future requests can simply call that skill instead of regenerating the code from scratch.