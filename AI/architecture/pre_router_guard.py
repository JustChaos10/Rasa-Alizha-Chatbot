"""
Pre-Router Guard

Language-agnostic dangerous content detection.
Runs BEFORE translation/routing to block harmful requests early.
Returns refusal in the input language (English or Arabic).
"""

import sys
import re
import logging
from typing import Optional, Tuple
from dataclasses import dataclass

# Fix Windows console encoding for emoji/unicode
if sys.stdout and hasattr(sys.stdout, 'reconfigure'):
    try:
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    except Exception:
        pass
if sys.stderr and hasattr(sys.stderr, 'reconfigure'):
    try:
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except Exception:
        pass

logger = logging.getLogger(__name__)


@dataclass
class GuardResult:
    """Result of guard check."""
    allowed: bool
    reason: str
    refusal_message: str  # In original language
    category: str = ""  # e.g., "violence", "weapons", "illegal"


# Dangerous patterns (language-agnostic keywords)
DANGEROUS_PATTERNS = {
    "weapons": [
        r"\b(make|build|create|construct|assemble)\b.*\b(bomb|explosive|weapon|gun|firearm)\b",
        r"\b(bomb|explosive|weapon)\b.*\b(make|build|create|how to)\b",
        r"\bhow to make.*\b(bomb|weapon|explosive|poison|drug)\b",
        r"\brecipe for.*\b(bomb|explosive|meth|cocaine)\b",
        r"\binstructions for.*\b(bomb|weapon|poison)\b",
    ],
    "violence": [
        r"\bhow to (kill|murder|harm|hurt|attack)\b",
        r"\b(kill|murder|assassinate)\b.*\b(person|someone|people)\b",
        r"\bhurt (someone|people|person)\b",
    ],
    "illegal": [
        r"\bhow to hack\b.*\b(account|password|bank|system)\b",
        r"\bsteal (money|identity|credit card)\b",
        r"\bhow to (launder|counterfeit)\b",
        r"\bhow to (break into|burglary)\b",
    ],
    "self_harm": [
        r"\bhow to (commit suicide|kill myself|end my life)\b",
        r"\bways to (die|hurt myself)\b",
    ],
}

# Arabic patterns (common dangerous queries in Arabic)
ARABIC_DANGEROUS_PATTERNS = {
    "weapons": [
        r"ŸÉŸäŸÅ.*ÿµŸÜÿπ.*(ŸÇŸÜÿ®ŸÑÿ©|ŸÖÿ™ŸÅÿ¨ÿ±ÿßÿ™|ÿ≥ŸÑÿßÿ≠)",
        r"ÿ∑ÿ±ŸäŸÇÿ©.*ÿµŸÜÿπ.*(ŸÇŸÜÿ®ŸÑÿ©|ŸÖÿ™ŸÅÿ¨ÿ±ÿßÿ™)",
        r"ÿµŸÜÿßÿπÿ©.*(ŸÇŸÜÿ®ŸÑÿ©|ÿ≥ŸÑÿßÿ≠|ŸÖÿ™ŸÅÿ¨ÿ±ÿßÿ™)",
    ],
    "violence": [
        r"ŸÉŸäŸÅ.*(ÿ£ŸÇÿ™ŸÑ|ÿßŸÇÿ™ŸÑ|ŸÜŸÇÿ™ŸÑ|ŸÇÿ™ŸÑ)",
        r"ÿ∑ÿ±ŸäŸÇÿ©.*ŸÇÿ™ŸÑ",
    ],
}

# Unicode Arabic patterns (preferred). These are kept separate so we don't
# depend on any mojibake/encoding artifacts in older pattern sets.
ARABIC_DANGEROUS_PATTERNS_UNICODE = {
    "weapons": [
        r"(?:ŸÉŸäŸÅ(?:Ÿäÿ©)?|ÿ∑ÿ±ŸäŸÇÿ©)\s+(?:ÿµŸÜÿπ|ÿπŸÖŸÑ|ÿ™ÿ≠ÿ∂Ÿäÿ±|ÿ™ÿ¨ŸáŸäÿ≤)\s+(?:ŸÇŸÜÿ®ŸÑÿ©|ŸÖÿ™ŸÅÿ¨ÿ±ÿßÿ™|ÿπÿ®Ÿàÿ©\s+ŸÜÿßÿ≥ŸÅÿ©)\b",
        r"\b(?:ÿßÿµŸÜÿπ|ÿ£ÿµŸÜÿπ|ÿµŸÜÿπ)\s+(?:ŸÇŸÜÿ®ŸÑÿ©|ŸÖÿ™ŸÅÿ¨ÿ±ÿßÿ™|ÿπÿ®Ÿàÿ©\s+ŸÜÿßÿ≥ŸÅÿ©)\b",
        r"\b(?:ŸÖŸÉŸàŸÜÿßÿ™|ŸÖŸàÿßÿØ)\s+(?:ŸÇŸÜÿ®ŸÑÿ©|ŸÖÿ™ŸÅÿ¨ÿ±ÿßÿ™|ÿπÿ®Ÿàÿ©\s+ŸÜÿßÿ≥ŸÅÿ©)\b",
    ],
    "violence": [
        r"\bŸÉŸäŸÅ\s+(?:ÿ£ŸÇÿ™ŸÑ|ÿßŸÇÿ™ŸÑ|ÿ£ÿ§ÿ∞Ÿä|ÿßÿ§ÿ∞Ÿä|ÿ£ÿ∂ÿ±|ÿßÿ∂ÿ±|ÿ£Ÿáÿßÿ¨ŸÖ|ÿßŸáÿßÿ¨ŸÖ)\b",
        r"(?:ŸÉŸäŸÅ|ÿ∑ÿ±ŸäŸÇÿ©)\s+(?:ŸÇÿ™ŸÑ|ÿ•Ÿäÿ∞ÿßÿ°|ÿßÿ∞Ÿäÿ©|ÿ•ÿ∂ÿ±ÿßÿ±|ÿßÿ∂ÿ±ÿßÿ±)\s+(?:ÿ¥ÿÆÿµ|ÿ•ŸÜÿ≥ÿßŸÜ|ÿ£ÿ≠ÿØ|ÿßÿ≠ÿØ)\b",
    ],
    "illegal": [
        r"\bŸÉŸäŸÅ\s+(?:ÿ£ÿÆÿ™ÿ±ŸÇ|ÿßÿÆÿ™ÿ±ŸÇ|ÿ£ŸáŸÉÿ±|ÿßŸáŸÉÿ±)\b",
        r"(?:ÿ∑ÿ±ŸäŸÇÿ©|ŸÉŸäŸÅ)\s+(?:ÿßÿÆÿ™ÿ±ÿßŸÇ|ÿ™ŸáŸÉŸäÿ±)\s+(?:ÿ≠ÿ≥ÿßÿ®|ÿ®ÿ±ŸäÿØ|ŸÜÿ∏ÿßŸÖ|ÿ¥ÿ®ŸÉÿ©)\b",
    ],
}

# Refusal messages
REFUSAL_EN = """I can't help with that request. This type of content could cause harm.

If you're experiencing difficult thoughts, please reach out to:
‚Ä¢ Emergency: 911
‚Ä¢ Crisis helpline: 988 (US)

Is there something else I can help you with?"""

REFUSAL_AR = """ŸÑÿß ÿ£ÿ≥ÿ™ÿ∑Ÿäÿπ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ© ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿ∑ŸÑÿ®. Ÿáÿ∞ÿß ÿßŸÑŸÜŸàÿπ ŸÖŸÜ ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ ŸÇÿØ Ÿäÿ≥ÿ®ÿ® ÿ∂ÿ±ÿ±Ÿãÿß.

ÿ•ÿ∞ÿß ŸÉŸÜÿ™ ÿ™ŸÖÿ± ÿ®ÿ£ŸÅŸÉÿßÿ± ÿµÿπÿ®ÿ©ÿå Ÿäÿ±ÿ¨Ÿâ ÿßŸÑÿ™ŸàÿßÿµŸÑ ŸÖÿπ:
‚Ä¢ ÿßŸÑÿ∑Ÿàÿßÿ±ÿ¶: 911
‚Ä¢ ÿÆÿ∑ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ© ÿßŸÑŸÜŸÅÿ≥Ÿäÿ©

ŸáŸÑ ŸäŸÖŸÉŸÜŸÜŸä ŸÖÿ≥ÿßÿπÿØÿ™ŸÉ ŸÅŸä ÿ¥Ÿäÿ° ÿ¢ÿÆÿ±ÿü"""


REFUSAL_AR_UNICODE = """ŸÑÿß ÿ£ÿ≥ÿ™ÿ∑Ÿäÿπ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ© ŸÅŸä Ÿáÿ∞ÿß ÿßŸÑÿ∑ŸÑÿ® ŸÑÿ£ŸÜŸá ŸÇÿØ Ÿäÿ≥ÿ®ÿ® ÿ∂ÿ±ÿ±Ÿãÿß.

ÿ•ÿ∞ÿß ŸÉŸÜÿ™ ŸÅŸä ÿÆÿ∑ÿ± ÿ£Ÿà ÿ™ŸÖÿ±Ÿë ÿ®ÿ£ÿ≤ŸÖÿ©ÿå ÿ™ŸàÿßÿµŸÑ ŸÖÿπ ÿÆÿØŸÖÿßÿ™ ÿßŸÑÿ∑Ÿàÿßÿ±ÿ¶ ÿßŸÑŸÖÿ≠ŸÑŸäÿ© ÿ£Ÿà ÿÆÿ∑ ÿßŸÑŸÖÿ≥ÿßÿπÿØÿ© ŸÅŸä ÿ®ŸÑÿØŸÉ.

ŸáŸÑ ŸäŸÖŸÉŸÜŸÜŸä ŸÖÿ≥ÿßÿπÿØÿ™ŸÉ ŸÅŸä ÿ¥Ÿäÿ° ÿ¢ÿÆÿ±ÿü"""


class PreRouterGuard:
    """
    Pre-routing guard for dangerous content.
    
    Usage:
        guard = PreRouterGuard()
        result = guard.check(message, detected_language)
        if not result.allowed:
            return result.refusal_message
    """
    
    def __init__(self, strict_mode: bool = True):
        """
        Initialize guard.
        
        Args:
            strict_mode: If True, be more aggressive in blocking
        """
        self.strict_mode = strict_mode
        self._compile_patterns()
    
    def _compile_patterns(self):
        """Compile regex patterns for faster matching."""
        self._compiled_en = {}
        for category, patterns in DANGEROUS_PATTERNS.items():
            self._compiled_en[category] = [
                re.compile(p, re.IGNORECASE) for p in patterns
            ]
        
        self._compiled_ar = {}
        for category, patterns in ARABIC_DANGEROUS_PATTERNS.items():
            self._compiled_ar[category] = [
                re.compile(p, re.UNICODE) for p in patterns
            ]

        # Preferred Unicode Arabic patterns
        for category, patterns in ARABIC_DANGEROUS_PATTERNS_UNICODE.items():
            compiled = [re.compile(p, re.UNICODE) for p in patterns]
            if category in self._compiled_ar:
                self._compiled_ar[category].extend(compiled)
            else:
                self._compiled_ar[category] = compiled
    
    def check(self, message: str, detected_language: str = "en") -> GuardResult:
        """
        Check if message contains dangerous content.
        
        Args:
            message: User's message
            detected_language: Detected language ("en" or "ar")
            
        Returns:
            GuardResult with allowed status and refusal if blocked
        """
        if not message:
            return GuardResult(allowed=True, reason="", refusal_message="")
        
        message_lower = message.lower().strip()
        
        # Check English patterns
        for category, patterns in self._compiled_en.items():
            for pattern in patterns:
                if pattern.search(message_lower):
                    logger.warning(f"üõ°Ô∏è Guard blocked: category={category}, pattern matched")
                    return GuardResult(
                        allowed=False,
                        reason=f"Dangerous content detected: {category}",
                        refusal_message=REFUSAL_AR_UNICODE if detected_language == "ar" else REFUSAL_EN,
                        category=category
                    )
        
        # Check Arabic patterns (if text contains Arabic)
        if any('\u0600' <= c <= '\u06FF' for c in message):
            for category, patterns in self._compiled_ar.items():
                for pattern in patterns:
                    if pattern.search(message):
                        logger.warning(f"üõ°Ô∏è Guard blocked (AR): category={category}")
                        return GuardResult(
                            allowed=False,
                            reason=f"Dangerous content detected: {category}",
                            refusal_message=REFUSAL_AR_UNICODE,
                            category=category
                        )
        
        return GuardResult(allowed=True, reason="", refusal_message="")
    
    def is_safe(self, message: str, detected_language: str = "en") -> bool:
        """Quick check if message is safe to process."""
        return self.check(message, detected_language).allowed


# Singleton accessor
_guard_instance: Optional[PreRouterGuard] = None

def get_pre_router_guard() -> PreRouterGuard:
    """Get the pre-router guard singleton."""
    global _guard_instance
    if _guard_instance is None:
        _guard_instance = PreRouterGuard()
    return _guard_instance
